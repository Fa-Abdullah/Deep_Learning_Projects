# Deep Learning and Artificial Intelligence Tasks

This repository contains a structured collection of deep learning and machine learning tasks implemented using Python and modern AI frameworks.  
The repository covers core concepts in neural networks, computer vision, natural language processing, sequence modeling, generative models, and transformer-based architectures.

All tasks are organized within a single repository for clarity and ease of navigation, with each task implemented in a separate Jupyter Notebook.

---

## Repository Structure

- Task 1: Neural Networks  
- Task 2: CNN and Transfer Learning  
- Task 3: NLP Preprocessing  
- Task 4: RNN, LSTM, GRU, and BiLSTM  
- Task 5: Sequence-to-Sequence Models  
- Task 6: Autoencoders and GANs  
- Task 7: Hugging Face Transformers  

---

## Task Details

### Task 1: Neural Networks
Datasets:
- Rent Dataset (Regression)
- Stroke Dataset (Classification)

Implementation Details:
- Regression using fully connected neural networks
- Binary classification using Sigmoid activation
- Multi-class classification using Softmax activation
- Focus on model optimization, evaluation metrics, and clean notebook structure

---

### Task 2: CNN and Transfer Learning
Datasets:
- CT Images
- Chest X-Ray (CXR)
- Cough Sound Data

Implementation Details:
- Convolutional Neural Networks for image-based classification
- Transfer learning using pretrained models
- Early fusion approach
- Intermediate fusion approach
- Single-modality classification using CT, CXR, or Cough data

---

### Task 3: NLP Preprocessing
Dataset:
- Arabic Sentiment Twitter Corpus

Applied Techniques:
- Text cleaning and normalization
- Tokenization
- Count Vectorization
- N-grams
- TF-IDF feature extraction

Note:
Implementation may vary based on team size and task constraints.

---

### Task 4: Recurrent Neural Networks
Dataset:
- Arabic Sentiment Twitter Corpus

Implementation Details:
- Reuse of preprocessing pipeline from Task 3
- Models implemented include RNN, LSTM, GRU, and BiLSTM
- Achieved accuracy exceeding 90 percent according to task requirements

---

### Task 5: Sequence-to-Sequence Models
Applications:
- Sign Language Translation
- English to Indian Language Translation

Key Concepts:
- Encoder-decoder architecture
- Sequence modeling for translation tasks

---

### Task 6: Autoencoders and Generative Adversarial Networks
Dataset:
- 50K Celebrity Faces

Implementation Details:
- Autoencoder for image reconstruction
- Generative Adversarial Networks for image generation
- Focus on latent space representation and training stability

---

### Task 7: Hugging Face Transformers
Overview:
This task focuses on using pretrained transformer models for natural language processing tasks.

Implementation Details:
- Loading pretrained models using the Hugging Face Transformers library
- Tokenization using Hugging Face tokenizers
- Fine-tuning transformer-based models
- Model evaluation using standard NLP metrics

---

## Technologies Used
- Python
- PyTorch
- TensorFlow
- Keras
- NumPy
- Pandas
- Matplotlib
- Scikit-learn
- NLP libraries

---

## Notes
- Datasets are not included due to size limitations
- Dataset links and setup instructions are provided inside each task folder
- All implementations are provided as Jupyter Notebooks
- This repository is intended for educational and practical implementation purposes
